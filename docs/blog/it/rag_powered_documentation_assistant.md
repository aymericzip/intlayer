---
createdAt: 2025-09-10
updatedAt: 2025-09-10
title: Costruire un Assistente alla Documentazione Potenziato da RAG (Chunking, Embeddings e Ricerca)
description: Costruire un Assistente alla Documentazione Potenziato da RAG (Chunking, Embeddings e Ricerca)
keywords:
  - RAG
  - Documentazione
  - Assistente
  - Chunking
  - Embeddings
  - Ricerca
slugs:
  - blog
  - rag-powered-documentation-assistant
---

# Costruire un Assistente alla Documentazione Potenziato da RAG (Chunking, Embeddings e Ricerca)

## Cosa ottieni

Ho costruito un assistente alla documentazione potenziato da RAG e l'ho confezionato in un boilerplate che puoi usare immediatamente.

- Include un'applicazione pronta all'uso (Next.js + OpenAI API)
- Include una pipeline RAG funzionante (chunking, embeddings, similarit√† coseno)
- Fornisce un'interfaccia chatbot completa costruita in React
- Tutti i componenti UI sono completamente modificabili con Tailwind CSS
- Registra ogni query dell'utente per aiutare a identificare documentazione mancante, punti dolenti degli utenti e opportunit√† di prodotto

üëâ¬†[Demo live](https://intlayer.org/doc/why) üëâ¬†[Boilerplate del codice](https://github.com/aymericzip/smart_doc_RAG)

## Introduzione

Se ti sei mai perso nella documentazione, scorrendo all'infinito alla ricerca di una risposta, sai quanto pu√≤ essere frustrante. La documentazione √® utile, ma √® statica e la ricerca spesso risulta macchinosa.

Ed √® qui che entra in gioco¬†**RAG (Retrieval-Augmented Generation)**. Invece di costringere gli utenti a scavare nel testo, possiamo combinare¬†**retrieval**¬†(trovare le parti giuste della documentazione) con¬†**generation**¬†(permettendo a un LLM di spiegarla in modo naturale).

In questo post, ti guider√≤ attraverso il modo in cui ho costruito un chatbot per la documentazione alimentato da RAG e come questo non solo aiuta gli utenti a trovare risposte pi√π velocemente, ma offre anche ai team di prodotto un nuovo modo per comprendere i punti dolenti degli utenti.

## Perch√© usare RAG per la documentazione?

RAG √® diventato un approccio popolare per una ragione: √® uno dei modi pi√π pratici per rendere i modelli di linguaggio di grandi dimensioni realmente utili.

Per la documentazione, i vantaggi sono chiari:

- Risposte istantanee: gli utenti chiedono in linguaggio naturale e ricevono risposte pertinenti.
- Contesto migliore: il modello vede solo le sezioni della documentazione pi√π rilevanti, riducendo le allucinazioni.
- Ricerca che sembra umana: pi√π simile a una combinazione di Algolia + FAQ + chatbot.
- Ciclo di feedback: memorizzando le query, si scopre ci√≤ con cui gli utenti hanno davvero difficolt√†.

Quel ultimo punto √® cruciale. Un sistema RAG non si limita a rispondere alle domande, ma ti dice cosa le persone stanno chiedendo. Questo significa:

- Scopri informazioni mancanti nella tua documentazione.
- Vedi emergere richieste di funzionalit√†.
- Individui schemi che possono persino guidare la strategia di prodotto.

Quindi, RAG non √® solo uno strumento di supporto. √à anche un **motore di scoperta del prodotto**.

## Come funziona la pipeline RAG

![RAG Pipeline](https://github.com/aymericzip/intlayer/blob/main/docs/assets/rag_flow.svg)

Ad alto livello, ecco la ricetta che ho usato:

1.  **Suddivisione della documentazione in chunk** Grandi file Markdown vengono divisi in chunk. La suddivisione permette di fornire come contesto solo le parti rilevanti della documentazione.
2.  **Generazione degli embeddings** Ogni chunk viene trasformato in un vettore usando l‚ÄôAPI di embedding di OpenAI (text-embedding-3-large) o un database vettoriale (Chroma, Qdrant, Pinecone).
3.  **Indicizzazione e memorizzazione** Gli embeddings vengono memorizzati in un semplice file JSON (per la mia demo), ma in produzione probabilmente useresti un database vettoriale.
4.  **Recupero (R in RAG)** La query dell‚Äôutente viene trasformata in embedding, si calcola la similarit√† coseno e si recuperano i chunk pi√π pertinenti.
5.  **Aumento + Generazione (AG in RAG)** Quei chunk vengono inseriti nel prompt per ChatGPT, cos√¨ il modello risponde con il contesto reale della documentazione.
6.  **Registrazione delle query per feedback** Ogni query utente viene memorizzata. Questo √® prezioso per capire i punti critici, la documentazione mancante o nuove opportunit√†.

## Passo 1: Lettura della Documentazione

Il primo passo √® stato semplice: avevo bisogno di un modo per scansionare una cartella docs/ per tutti i file .md. Usando Node.js e glob, ho recuperato il contenuto di ogni file Markdown in memoria.

Questo mantiene la pipeline flessibile: invece di Markdown, potresti recuperare la documentazione da un database, un CMS o anche un'API.

## Passo 2: Suddividere la Documentazione in Chunk

Perch√© suddividere in chunk? Perch√© i modelli linguistici hanno **limiti di contesto**. Fornire loro un intero libro di documentazione non funzioner√†.

Quindi l'idea √® di spezzare il testo in chunk gestibili (ad esempio 500 token ciascuno) con sovrapposizione (ad esempio 100 token). La sovrapposizione garantisce continuit√† cos√¨ da non perdere il significato ai confini dei chunk.

**Esempio:**

- Chunk 1 ‚Üí ‚Äú‚Ä¶la vecchia biblioteca che molti avevano dimenticato. Le sue scaffalature imponenti erano piene di libri‚Ä¶‚Äù
- Chunk 2 ‚Üí ‚Äú‚Ä¶le scaffalature erano piene di libri di ogni genere immaginabile, ognuno sussurrando storie‚Ä¶‚Äù

La sovrapposizione assicura che entrambi i chunk contengano un contesto condiviso, cos√¨ il recupero rimane coerente.

Questo compromesso (dimensione del chunk vs sovrapposizione) √® fondamentale per l‚Äôefficienza di RAG:

- Troppo piccolo ‚Üí si genera rumore.
- Troppo grande ‚Üí si satura la dimensione del contesto.

## Passo 3: Generazione degli Embeddings

Una volta che i documenti sono stati suddivisi in chunk, generiamo gli **embeddings** ‚Äî vettori ad alta dimensione che rappresentano ogni chunk.

Ho utilizzato il modello text-embedding-3-large di OpenAI, ma potresti usare qualsiasi modello moderno di embedding.

**Esempio di embedding:**

```js
[
  -0.0002630692, -0.029749284, 0.010225477, -0.009224428, -0.0065269712,
  -0.002665544, 0.003214777, 0.04235309, -0.033162255, -0.00080789323,
  //...+1533 elementi
];
```

Ogni vettore √® un‚Äôimpronta matematica del testo, che permette la ricerca per similarit√†.

## Passo 4: Indicizzazione e Memorizzazione degli Embeddings

Per evitare di rigenerare gli embeddings pi√π volte, li ho memorizzati in embeddings.json.

In produzione, probabilmente vorresti un database vettoriale come:

- Chroma
- Qdrant
- Pinecone
- FAISS, Weaviate, Milvus, ecc.

I database vettoriali gestiscono indicizzazione, scalabilit√† e ricerca veloce. Ma per il mio prototipo, un JSON locale ha funzionato bene.

## Passo 5: Recupero con Similarit√† Coseno

Quando un utente fa una domanda:

1. Genera un embedding per la query.
2. Confrontalo con tutti gli embedding dei documenti usando la **similarit√† coseno**.
3. Conserva solo i primi N chunk pi√π simili.

La similarit√† coseno misura l'angolo tra due vettori. Una corrispondenza perfetta ottiene un punteggio di **1.0**.

In questo modo, il sistema trova i passaggi del documento pi√π vicini alla query.

## Passo 6: Aggiunta + Generazione

Ora arriva la magia. Prendiamo i chunk migliori e li inseriamo nel **prompt di sistema** per ChatGPT.

Ci√≤ significa che il modello risponde come se quei frammenti facessero parte della conversazione.

Il risultato: risposte accurate e **basate sulla documentazione**.

## Passo 7: Registrazione delle Query degli Utenti

Questa √® la superpotenza nascosta.

Ogni domanda posta viene memorizzata. Nel tempo, si costruisce un dataset di:

- Domande pi√π frequenti (ottimo per le FAQ)
- Domande senza risposta (documentazione mancante o poco chiara)
- Richieste di funzionalit√† mascherate da domande (‚ÄúSi integra con X?‚Äù)
- Casi d‚Äôuso emergenti che non avevi previsto

Questo trasforma il tuo assistente RAG in uno **strumento continuo di ricerca utenti**.

## Quanto Costa?

Una comune obiezione al RAG √® il costo. In pratica, √® sorprendentemente economico:

- Generare embedding per circa 200 documenti richiede circa **5 minuti** e costa **1‚Äì2 euro**.
- La funzione di ricerca nei documenti √® completamente gratuita.
- Per le query, utilizziamo gpt-4o-latest senza la modalit√† ‚Äúthinking‚Äù. Su Intlayer, registriamo circa **300 query chat al mese**, e la fattura dell‚ÄôAPI OpenAI raramente supera i **10$**.

Oltre a questo, puoi includere il costo dell‚Äôhosting.

## Dettagli di Implementazione

Stack:

- Monorepo: workspace pnpm
- Pacchetto Doc: Node.js / TypeScript / OpenAI API
- Frontend: Next.js / React / Tailwind CSS
- Backend: route API Node.js / OpenAI API

Il pacchetto `@smart-doc/docs` √® un pacchetto TypeScript che gestisce l‚Äôelaborazione della documentazione. Quando un file markdown viene aggiunto o modificato, il pacchetto include uno script `build` che ricostruisce la lista della documentazione in ogni lingua, genera gli embeddings e li memorizza in un file `embeddings.json`.

Per il frontend, utilizziamo un‚Äôapplicazione Next.js che fornisce:

- Rendering da Markdown a HTML
- Una barra di ricerca per trovare la documentazione rilevante
- Un'interfaccia chatbot per porre domande sulla documentazione

Per eseguire una ricerca nella documentazione, l'applicazione Next.js include una route API che chiama una funzione nel pacchetto `@smart-doc/docs` per recuperare i frammenti di documentazione corrispondenti alla query. Utilizzando questi frammenti, possiamo restituire una lista di pagine della documentazione rilevanti per la ricerca dell'utente.

Per la funzionalit√† chatbot, seguiamo lo stesso processo di ricerca ma in aggiunta iniettiamo i frammenti di documentazione recuperati nel prompt inviato a ChatGPT.

Ecco un esempio di prompt inviato a ChatGPT:

Prompt di sistema:

```txt
Sei un assistente utile che pu√≤ rispondere a domande sulla documentazione di Intlayer.

Frammenti correlati:

-----
docName: "getting-started"
docChunk: "1/3"
docUrl: "https://example.com/docs/it/getting-started"
---

# Come iniziare

...

-----
docName: "another-doc"
docChunk: "1/5"
docUrl: "https://example.com/docs/it/another-doc"
---

# Un altro documento

...
```

Query utente :

```txt
Come iniziare?
```

Usiamo SSE per trasmettere la risposta dalla route API.

Come accennato, utilizziamo gpt-4-turbo senza la modalit√† "thinking". Le risposte sono pertinenti e la latenza √® bassa.
Abbiamo sperimentato con gpt-5, ma la latenza era troppo alta (a volte fino a 15 secondi per una risposta). Tuttavia, lo rivaluteremo in futuro.

üëâ¬†[Prova la demo qui](https://intlayer.org/doc/why) üëâ¬†[Controlla il modello di codice su GitHub](https://github.com/aymericzip/smart_doc_RAG)

## Andare oltre

Questo progetto √® una implementazione minima. Ma puoi estenderlo in molti modi:

- Server MCP ‚Üí la funzione di ricerca nella documentazione verso un server MCP per connettere la documentazione a qualsiasi assistente AI

- Database vettoriali ‚Üí scalare a milioni di frammenti di documenti
- LangChain / LlamaIndex ‚Üí framework pronti all'uso per pipeline RAG
- Cruscotti di analisi ‚Üí visualizzare le query degli utenti e i punti critici
- Recupero multi-sorgente ‚Üí estrarre non solo documenti, ma anche voci di database, post di blog, ticket, ecc.
- Prompting migliorato ‚Üí riordinamento, filtraggio e ricerca ibrida (parola chiave + semantica)

## Limitazioni che abbiamo incontrato

- La suddivisione in frammenti e la sovrapposizione sono empiriche. Il giusto equilibrio (dimensione del frammento, percentuale di sovrapposizione, numero di frammenti recuperati) richiede iterazione e test.
- Gli embeddings non vengono rigenerati automaticamente quando i documenti cambiano. Il nostro sistema resetta gli embeddings per un file solo se il numero di frammenti differisce da quello memorizzato.
- In questo prototipo, gli embeddings sono memorizzati in JSON. Questo funziona per le demo ma inquina Git. In produzione, √® meglio utilizzare un database o un archivio vettoriale dedicato.

## Perch√© Questo √à Importante Oltre la Documentazione

La parte interessante non √® solo il chatbot. √à il **ciclo di feedback**.

Con RAG, non ti limiti a rispondere:

- Impari cosa confonde gli utenti.
- Scopri quali funzionalit√† si aspettano.
- Adatti la tua strategia di prodotto basandoti sulle query reali.

**Esempio:**

Immagina di lanciare una nuova funzionalit√† e vedere immediatamente:

- Il 50% delle domande riguarda lo stesso passaggio di configurazione poco chiaro
- Gli utenti chiedono ripetutamente un‚Äôintegrazione che ancora non supporti
- Le persone cercano termini che rivelano un nuovo caso d‚Äôuso

Questa √® **intelligenza di prodotto** direttamente dai tuoi utenti.

## Conclusione

RAG √® uno dei modi pi√π semplici e potenti per rendere pratici i LLM. Combinando **recupero + generazione**, puoi trasformare documenti statici in un **assistente intelligente** e, allo stesso tempo, ottenere un flusso continuo di approfondimenti sul prodotto.

Per me, questo progetto ha dimostrato che RAG non √® solo un trucco tecnico. √à un modo per trasformare la documentazione in:

- un sistema di supporto interattivo
- un canale di feedback
- uno strumento di strategia di prodotto

üëâ [Prova la demo qui](https://intlayer.org/doc/why) üëâ [Controlla il modello di codice su GitHub](https://github.com/aymericzip/smart_doc_RAG)

E se anche tu stai sperimentando con RAG, mi piacerebbe sapere come lo stai usando.
