---
createdAt: 2025-09-10
updatedAt: 2025-09-10
title: Construyendo un Asistente de Documentaci√≥n Potenciado por RAG (Fragmentaci√≥n, Embeddings y B√∫squeda)
description: Construyendo un Asistente de Documentaci√≥n Potenciado por RAG (Fragmentaci√≥n, Embeddings y B√∫squeda)
keywords:
  - RAG
  - Documentaci√≥n
  - Asistente
  - Fragmentaci√≥n
  - Embeddings
  - B√∫squeda
slugs:
  - blog
  - rag-powered-documentation-assistant
---

# Construyendo un Asistente de Documentaci√≥n Potenciado por RAG (Fragmentaci√≥n, Embeddings y B√∫squeda)

## Lo que obtienes

He construido un asistente de documentaci√≥n potenciado por RAG y lo empaquet√© en una plantilla que puedes usar de inmediato.

- Viene con una aplicaci√≥n lista para usar (Next.js + API de OpenAI)
- Incluye una pipeline RAG funcional (fragmentaci√≥n, embeddings, similitud coseno)
- Proporciona una interfaz completa de chatbot construida en React
- Todos los componentes de la interfaz de usuario son completamente editables con Tailwind CSS
- Registra cada consulta del usuario para ayudar a identificar documentaci√≥n faltante, puntos de dolor del usuario y oportunidades de producto

üëâ¬†[Demostraci√≥n en vivo](https://intlayer.org/doc/why) üëâ¬†[Plantilla de c√≥digo](https://github.com/aymericzip/smart_doc_RAG)

## Introducci√≥n

Si alguna vez te has perdido en la documentaci√≥n, desplaz√°ndote sin fin en busca de una respuesta, sabes lo doloroso que puede ser. La documentaci√≥n es √∫til, pero es est√°tica y buscar en ella a menudo se siente torpe.

Ah√≠ es donde entra¬†**RAG (Generaci√≥n Aumentada por Recuperaci√≥n)**. En lugar de obligar a los usuarios a buscar entre el texto, podemos combinar¬†**recuperaci√≥n**¬†(encontrar las partes correctas de la documentaci√≥n) con¬†**generaci√≥n**¬†(dejar que un LLM lo explique de forma natural).

En esta publicaci√≥n, te guiar√© a trav√©s de c√≥mo constru√≠ un chatbot de documentaci√≥n potenciado por RAG y c√≥mo no solo ayuda a los usuarios a encontrar respuestas m√°s r√°pido, sino que tambi√©n ofrece a los equipos de producto una nueva forma de entender los puntos de dolor de los usuarios.

## ¬øPor qu√© usar RAG para la documentaci√≥n?

RAG se ha convertido en un enfoque popular por una raz√≥n: es una de las formas m√°s pr√°cticas de hacer que los grandes modelos de lenguaje sean realmente √∫tiles.

Para la documentaci√≥n, los beneficios son claros:

- Respuestas instant√°neas: los usuarios preguntan en lenguaje natural y obtienen respuestas relevantes.
- Mejor contexto: el modelo solo ve las secciones de documentaci√≥n m√°s relevantes, reduciendo las alucinaciones.
- B√∫squeda que se siente humana: m√°s como Algolia + FAQ + chatbot, todo en uno.
- Ciclo de retroalimentaci√≥n: al almacenar las consultas, descubres con qu√© realmente tienen dificultades los usuarios.

Ese √∫ltimo punto es crucial. Un sistema RAG no solo responde preguntas, sino que te dice qu√© es lo que la gente est√° preguntando. Eso significa:

- Descubres informaci√≥n faltante en tus documentos.
- Ves solicitudes de funciones emergentes.
- Detectas patrones que incluso pueden guiar la estrategia del producto.

As√≠ que, RAG no es solo una herramienta de soporte. Tambi√©n es un **motor de descubrimiento de producto**.

## C√≥mo funciona la canalizaci√≥n RAG

A grandes rasgos, esta es la receta que us√©:

1.  **Dividir la documentaci√≥n en fragmentos** Los archivos grandes de Markdown se dividen en fragmentos. Dividir permite proporcionar como contexto solo las partes relevantes de la documentaci√≥n.
2.  **Generar embeddings** Cada fragmento se convierte en un vector usando la API de embeddings de OpenAI (text-embedding-3-large) o una base de datos vectorial (Chroma, Qdrant, Pinecone).
3.  **Indexaci√≥n y almacenamiento** Los embeddings se almacenan en un archivo JSON simple (para mi demo), pero en producci√≥n, probablemente usar√≠as una base de datos vectorial.
4.  **Recuperaci√≥n (R en RAG)** Se incrusta la consulta del usuario, se calcula la similitud coseno y se recuperan los fragmentos que mejor coinciden.
5.  **Aumento + Generaci√≥n (AG en RAG)** Esos fragmentos se inyectan en el prompt para ChatGPT, de modo que el modelo responda con el contexto real de la documentaci√≥n.
6.  **Registro de consultas para retroalimentaci√≥n** Cada consulta del usuario se almacena. Esto es oro para entender puntos problem√°ticos, documentaci√≥n faltante o nuevas oportunidades.

## Paso 1: Leer la documentaci√≥n

El primer paso fue simple: necesitaba una forma de escanear una carpeta docs/ para todos los archivos .md. Usando Node.js y glob, obtuve el contenido de cada archivo Markdown en memoria.

Esto mantiene la canalizaci√≥n flexible: en lugar de Markdown, podr√≠as obtener documentos de una base de datos, un CMS o incluso una API.

## Paso 2: Dividir la Documentaci√≥n en Fragmentos

¬øPor qu√© dividir en fragmentos? Porque los modelos de lenguaje tienen **l√≠mites de contexto**. Alimentarlos con un libro entero de documentaci√≥n no funcionar√°.

Entonces, la idea es dividir el texto en fragmentos manejables (por ejemplo, 500 tokens cada uno) con solapamiento (por ejemplo, 100 tokens). El solapamiento asegura continuidad para que no pierdas el significado en los l√≠mites de los fragmentos.

**Ejemplo:**

- Fragmento 1 ‚Üí ‚Äú‚Ä¶la vieja biblioteca que muchos hab√≠an olvidado. Sus estanter√≠as imponentes estaban llenas de libros‚Ä¶‚Äù
- Fragmento 2 ‚Üí ‚Äú‚Ä¶las estanter√≠as estaban llenas de libros de todos los g√©neros imaginables, cada uno susurrando historias‚Ä¶‚Äù

El solapamiento asegura que ambos fragmentos contengan contexto compartido, por lo que la recuperaci√≥n permanece coherente.

Este compromiso (tama√±o del fragmento vs superposici√≥n) es clave para la eficiencia de RAG:

- Demasiado peque√±o ‚Üí obtienes ruido.
- Demasiado grande ‚Üí explotas el tama√±o del contexto.

## Paso 3: Generaci√≥n de Embeddings

Una vez que los documentos est√°n fragmentados, generamos **embeddings** ‚Äî vectores de alta dimensi√≥n que representan cada fragmento.

Us√© el modelo text-embedding-3-large de OpenAI, pero podr√≠as usar cualquier modelo moderno de embeddings.

**Ejemplo de embedding:**

```js
[
  -0.0002630692, -0.029749284, 0.010225477, -0.009224428, -0.0065269712,
  -0.002665544, 0.003214777, 0.04235309, -0.033162255, -0.00080789323,
  //...+1533 elementos
];
```

Cada vector es una huella matem√°tica del texto, que permite la b√∫squeda por similitud.

## Paso 4: Indexaci√≥n y Almacenamiento de Embeddings

Para evitar regenerar embeddings m√∫ltiples veces, los almacen√© en embeddings.json.

En producci√≥n, probablemente querr√°s una base de datos vectorial como:

- Chroma
- Qdrant
- Pinecone
- FAISS, Weaviate, Milvus, etc.

Las bases de datos vectoriales manejan la indexaci√≥n, escalabilidad y b√∫squeda r√°pida. Pero para mi prototipo, un JSON local funcion√≥ bien.

## Paso 5: Recuperaci√≥n con Similitud del Coseno

Cuando un usuario hace una pregunta:

1. Generar un embedding para la consulta.
2. Compararlo con todos los embeddings de los documentos usando **similitud del coseno**.
3. Mantener solo los N fragmentos m√°s similares.

La similitud del coseno mide el √°ngulo entre dos vectores. Una coincidencia perfecta obtiene un puntaje de **1.0**.

De esta manera, el sistema encuentra los pasajes del documento m√°s cercanos a la consulta.

## Paso 6: Aumento + Generaci√≥n

Ahora viene la magia. Tomamos los fragmentos principales y los inyectamos en el **prompt del sistema** para ChatGPT.

Eso significa que el modelo responde como si esos fragmentos fueran parte de la conversaci√≥n.

El resultado: respuestas precisas y **basadas en la documentaci√≥n**.

## Paso 7: Registro de Consultas de Usuarios

Este es el superpoder oculto.

Cada pregunta realizada se almacena. Con el tiempo, construyes un conjunto de datos de:

- Preguntas m√°s frecuentes (ideal para FAQs)
- Preguntas sin respuesta (documentaci√≥n faltante o poco clara)
- Solicitudes de funciones disfrazadas de preguntas (‚Äú¬øSe integra con X?‚Äù)
- Casos de uso emergentes que no hab√≠as previsto

Esto convierte a tu asistente RAG en una **herramienta continua de investigaci√≥n de usuarios**.

## ¬øCu√°nto Cuesta?

Una objeci√≥n com√∫n a RAG es el costo. En la pr√°ctica, es sorprendentemente barato:

- Generar embeddings para ~200 documentos toma alrededor de **5 minutos** y cuesta **1‚Äì2 euros**.
- La funci√≥n de b√∫squeda en documentos es 100% gratuita.
- Para las consultas, usamos gpt-4o-latest sin el modo de ‚Äúpensamiento‚Äù. En Intlayer, vemos alrededor de **300 consultas de chat por mes**, y la factura de la API de OpenAI rara vez supera los **10 $**.

Adem√°s de eso, puedes incluir el costo de alojamiento.

## Detalles de Implementaci√≥n

Stack:

- Monorepo: espacio de trabajo pnpm
- Paquete de documentaci√≥n: Node.js / TypeScript / API de OpenAI
- Frontend: Next.js / React / Tailwind CSS
- Backend: ruta API de Node.js / API de OpenAI

El paquete `@smart-doc/docs` es un paquete TypeScript que maneja el procesamiento de la documentaci√≥n. Cuando se agrega o modifica un archivo markdown, el paquete incluye un script `build` que reconstruye la lista de documentaci√≥n en cada idioma, genera embeddings y los almacena en un archivo `embeddings.json`.

Para el frontend, usamos una aplicaci√≥n Next.js que proporciona:

- Renderizado de Markdown a HTML
- Una barra de b√∫squeda para encontrar documentaci√≥n relevante
- Una interfaz de chatbot para hacer preguntas sobre la documentaci√≥n

Para realizar una b√∫squeda en la documentaci√≥n, la aplicaci√≥n Next.js incluye una ruta API que llama a una funci√≥n del paquete `@smart-doc/docs` para recuperar fragmentos de documentaci√≥n que coincidan con la consulta. Usando estos fragmentos, podemos devolver una lista de p√°ginas de documentaci√≥n relevantes para la b√∫squeda del usuario.

Para la funcionalidad del chatbot, seguimos el mismo proceso de b√∫squeda pero adem√°s inyectamos los fragmentos de documentaci√≥n recuperados en el prompt enviado a ChatGPT.

Aqu√≠ hay un ejemplo de un prompt enviado a ChatGPT:

Prompt del sistema:

```txt
Eres un asistente √∫til que puede responder preguntas sobre la documentaci√≥n de Intlayer.

Fragmentos relacionados:

-----
docName: "getting-started"
docChunk: "1/3"
docUrl: "https://example.com/docs/es/getting-started"
---

# C√≥mo empezar

...

-----
docName: "another-doc"
docChunk: "1/5"
docUrl: "https://example.com/docs/es/another-doc"
---

# Otro documento

...
```

Consulta del usuario:

```txt
¬øC√≥mo empezar?
```

Usamos SSE para transmitir la respuesta desde la ruta API.

Como se mencion√≥, usamos gpt-4-turbo sin modo "pensando". Las respuestas son relevantes y la latencia es baja.
Experimentamos con gpt-5, pero la latencia era demasiado alta (a veces hasta 15 segundos para una respuesta). Pero lo revisaremos en el futuro.

üëâ¬†[Prueba la demo aqu√≠](https://intlayer.org/doc/why) üëâ¬†[Consulta la plantilla de c√≥digo en GitHub](https://github.com/aymericzip/smart_doc_RAG)

## Ir m√°s all√°

Este proyecto es una implementaci√≥n m√≠nima. Pero puedes ampliarlo de muchas maneras:

- Servidor MCP ‚Üí la funci√≥n de b√∫squeda en la documentaci√≥n a un servidor MCP para conectar la documentaci√≥n con cualquier asistente de IA

- Bases de datos vectoriales ‚Üí escalar a millones de fragmentos de documentaci√≥n
- LangChain / LlamaIndex ‚Üí frameworks listos para usar para pipelines RAG
- Paneles de an√°lisis ‚Üí visualizar consultas de usuarios y puntos problem√°ticos
- Recuperaci√≥n multisource ‚Üí obtener no solo documentos, sino entradas de bases de datos, publicaciones de blogs, tickets, etc.
- Mejora en el prompting ‚Üí reordenamiento, filtrado y b√∫squeda h√≠brida (palabra clave + sem√°ntica)

## Limitaciones que hemos encontrado

- La segmentaci√≥n y el solapamiento son emp√≠ricos. El equilibrio correcto (tama√±o del fragmento, porcentaje de solapamiento, n√∫mero de fragmentos recuperados) requiere iteraci√≥n y pruebas.
- Los embeddings no se regeneran autom√°ticamente cuando los documentos cambian. Nuestro sistema reinicia los embeddings para un archivo solo si el n√∫mero de fragmentos difiere de lo almacenado.
- En este prototipo, los embeddings se almacenan en JSON. Esto funciona para demostraciones pero contamina Git. En producci√≥n, es mejor usar una base de datos o un almac√©n vectorial dedicado.

## Por qu√© esto importa m√°s all√° de la documentaci√≥n

La parte interesante no es solo el chatbot. Es el **bucle de retroalimentaci√≥n**.

Con RAG, no solo respondes:

- Aprendes qu√© confunde a los usuarios.
- Descubres qu√© caracter√≠sticas esperan.
- Adaptas tu estrategia de producto bas√°ndote en consultas reales.

**Ejemplo:**

Imagina lanzar una nueva funci√≥n y ver instant√°neamente:

- El 50% de las preguntas son sobre el mismo paso de configuraci√≥n poco claro
- Los usuarios piden repetidamente una integraci√≥n que a√∫n no soportas
- La gente busca t√©rminos que revelan un nuevo caso de uso

Eso es **inteligencia de producto** directamente de tus usuarios.

## Conclusi√≥n

- En este prototipo, los embeddings se almacenan en JSON. Esto funciona para demostraciones pero contamina Git. En producci√≥n, es mejor usar una base de datos o un almac√©n vectorial dedicado.

## Por qu√© esto importa m√°s all√° de la documentaci√≥n

La parte interesante no es solo el chatbot. Es el **ciclo de retroalimentaci√≥n**.

Con RAG, no solo respondes:

- Aprendes qu√© confunde a los usuarios.
- Descubres qu√© funciones esperan.
- Adaptas tu estrategia de producto bas√°ndote en consultas reales.

**Ejemplo:**

Imagina lanzar una nueva funci√≥n y ver instant√°neamente:

- El 50% de las preguntas son sobre el mismo paso de configuraci√≥n poco claro
- Los usuarios piden repetidamente una integraci√≥n que a√∫n no soportas
- La gente busca t√©rminos que revelan un nuevo caso de uso

Eso es **inteligencia de producto** directamente de tus usuarios.

## Conclusi√≥n

RAG es una de las formas m√°s simples y poderosas de hacer que los LLMs sean pr√°cticos. Al combinar **recuperaci√≥n + generaci√≥n**, puedes convertir documentos est√°ticos en un **asistente inteligente** y, al mismo tiempo, obtener un flujo continuo de informaci√≥n sobre el producto.

Para m√≠, este proyecto mostr√≥ que RAG no es solo un truco t√©cnico. Es una forma de transformar la documentaci√≥n en:

- un sistema de soporte interactivo
- un canal de retroalimentaci√≥n
- una herramienta de estrategia de producto

üëâ [Prueba la demo aqu√≠](https://intlayer.org/doc/why) üëâ [Consulta la plantilla de c√≥digo en GitHub](https://github.com/aymericzip/smart_doc_RAG)

Y si tambi√©n est√°s experimentando con RAG, me encantar√≠a saber c√≥mo lo est√°s usando.
