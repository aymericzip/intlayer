---
createdAt: 2025-09-10
updatedAt: 2025-09-10
title: CrÃ©ation dâ€™un assistant de documentation propulsÃ© par RAG (Segmentation, Embeddings et Recherche)
description: CrÃ©ation dâ€™un assistant de documentation propulsÃ© par RAG (Segmentation, Embeddings et Recherche)
keywords:
  - RAG
  - Documentation
  - Assistant
  - Segmentation
  - Embeddings
  - Recherche
slugs:
  - blog
  - rag-powered-documentation-assistant
---

# CrÃ©ation dâ€™un assistant de documentation propulsÃ© par RAG (Segmentation, Embeddings et Recherche)

## Ce que vous obtenez

Jâ€™ai crÃ©Ã© un assistant de documentation propulsÃ© par RAG et lâ€™ai emballÃ© dans un boilerplate que vous pouvez utiliser immÃ©diatement.

- LivrÃ© avec une application prÃªte Ã  lâ€™emploi (Next.js + API OpenAI)
- Comprend une pipeline RAG fonctionnelle (segmentation, embeddings, similaritÃ© cosinus)
- Fournit une interface complÃ¨te de chatbot construite en React
- Tous les composants UI sont entiÃ¨rement modifiables avec Tailwind CSS
- Enregistre chaque requÃªte utilisateur pour aider Ã  identifier les documents manquants, les points de douleur des utilisateurs et les opportunitÃ©s produit

ğŸ‘‰Â [DÃ©mo en direct](https://intlayer.org/doc/why) ğŸ‘‰Â [Boilerplate de code](https://github.com/aymericzip/smart_doc_RAG)

## Introduction

Si vous vous Ãªtes dÃ©jÃ  perdu dans une documentation, Ã  faire dÃ©filer sans fin pour trouver une rÃ©ponse, vous savez Ã  quel point cela peut Ãªtre pÃ©nible. Les docs sont utiles, mais elles sont statiques et leur recherche est souvent maladroite.

Câ€™est lÃ  quâ€™intervientÂ **RAG (Retrieval-Augmented Generation)**. Au lieu de forcer les utilisateurs Ã  fouiller dans le texte, nous pouvons combiner la **rÃ©cupÃ©ration** (trouver les bonnes parties de la doc) avec la **gÃ©nÃ©ration** (laisser un LLM lâ€™expliquer naturellement).

Dans cet article, je vais vous expliquer comment jâ€™ai construit un chatbot de documentation alimentÃ© par RAG et comment il ne se contente pas dâ€™aider les utilisateurs Ã  trouver des rÃ©ponses plus rapidement, mais offre Ã©galement aux Ã©quipes produit une nouvelle maniÃ¨re de comprendre les points de douleur des utilisateurs.

## Pourquoi utiliser RAG pour la documentation ?

RAG est devenu une approche populaire pour une raison : câ€™est lâ€™une des faÃ§ons les plus pratiques de rendre les grands modÃ¨les de langage rÃ©ellement utiles.

Pour la documentation, les avantages sont clairs :

- RÃ©ponses instantanÃ©es : les utilisateurs posent des questions en langage naturel et obtiennent des rÃ©ponses pertinentes.
- Meilleur contexte : le modÃ¨le ne voit que les sections de documentation les plus pertinentes, rÃ©duisant ainsi les hallucinations.
- Recherche qui semble humaine : un mÃ©lange dâ€™Algolia + FAQ + chatbot, tout en un.
- Boucle de rÃ©troaction : en stockant les requÃªtes, vous dÃ©couvrez ce avec quoi les utilisateurs ont vraiment du mal.

Ce dernier point est crucial. Un systÃ¨me RAG ne se contente pas de rÃ©pondre aux questions, il vous indique ce que les gens demandent. Cela signifie :

- Vous dÃ©couvrez des informations manquantes dans votre documentation.
- Vous voyez apparaÃ®tre des demandes de fonctionnalitÃ©s.
- Vous repÃ©rez des tendances qui peuvent mÃªme orienter la stratÃ©gie produit.

Ainsi, RAG nâ€™est pas seulement un outil de support. Câ€™est aussi un **moteur de dÃ©couverte produit**.

## Comment fonctionne le pipeline RAG

Ã€ un niveau Ã©levÃ©, voici la recette que jâ€™ai utilisÃ©e :

1.  **DÃ©coupage de la documentation** De gros fichiers Markdown sont divisÃ©s en morceaux. Le dÃ©coupage permet de fournir comme contexte uniquement les parties pertinentes de la documentation.
2.  **GÃ©nÃ©ration des embeddings** Chaque morceau est transformÃ© en vecteur en utilisant lâ€™API dâ€™embeddings dâ€™OpenAI (text-embedding-3-large) ou une base de donnÃ©es vectorielle (Chroma, Qdrant, Pinecone).
3.  **Indexation et stockage** Les embeddings sont stockÃ©s dans un simple fichier JSON (pour ma dÃ©mo), mais en production, vous utiliseriez probablement une base de donnÃ©es vectorielle.
4.  **RÃ©cupÃ©ration (R dans RAG)** Une requÃªte utilisateur est transformÃ©e en embedding, la similaritÃ© cosinus est calculÃ©e, et les chunks les plus pertinents sont rÃ©cupÃ©rÃ©s.
5.  **Augmentation + GÃ©nÃ©ration (AG dans RAG)** Ces chunks sont injectÃ©s dans le prompt pour ChatGPT, afin que le modÃ¨le rÃ©ponde avec le contexte rÃ©el de la documentation.
6.  **Enregistrement des requÃªtes pour feedback** Chaque requÃªte utilisateur est stockÃ©e. Câ€™est une mine dâ€™or pour comprendre les points de douleur, les documents manquants ou les nouvelles opportunitÃ©s.

## Ã‰tape 1 : Lecture des Docs

La premiÃ¨re Ã©tape Ã©tait simple : jâ€™avais besoin dâ€™un moyen pour scanner un dossier docs/ Ã  la recherche de tous les fichiers .md. En utilisant Node.js et glob, jâ€™ai rÃ©cupÃ©rÃ© le contenu de chaque fichier Markdown en mÃ©moire.

Cela maintient la flexibilitÃ© du pipeline : au lieu de Markdown, vous pourriez rÃ©cupÃ©rer les documents depuis une base de donnÃ©es, un CMS, ou mÃªme une API.

## Ã‰tape 2 : DÃ©coupage de la documentation

Pourquoi dÃ©couper ? Parce que les modÃ¨les de langage ont des **limites de contexte**. Leur fournir un livre entier de documentation ne fonctionnera pas.

Lâ€™idÃ©e est donc de diviser le texte en morceaux gÃ©rables (par exemple 500 tokens chacun) avec un chevauchement (par exemple 100 tokens). Le chevauchement assure la continuitÃ© pour ne pas perdre le sens aux frontiÃ¨res des morceaux.

**Exemple :**

- Morceau 1 â†’ Â« â€¦la vieille bibliothÃ¨que que beaucoup avaient oubliÃ©e. Ses Ã©tagÃ¨res imposantes Ã©taient remplies de livresâ€¦ Â»
- Morceau 2 â†’ Â« â€¦les Ã©tagÃ¨res Ã©taient remplies de livres de tous genres imaginables, chacun murmurant des histoiresâ€¦ Â»

Le chevauchement garantit que les deux morceaux contiennent un contexte partagÃ©, ce qui rend la rÃ©cupÃ©ration cohÃ©rente.

Ce compromis (taille des chunks vs chevauchement) est essentiel pour l'efficacitÃ© du RAG :

- Trop petit â†’ vous obtenez du bruit.
- Trop grand â†’ vous explosez la taille du contexte.

## Ã‰tape 3 : GÃ©nÃ©ration des embeddings

Une fois les documents dÃ©coupÃ©s en chunks, nous gÃ©nÃ©rons des **embeddings** â€” des vecteurs de haute dimension reprÃ©sentant chaque chunk.

J'ai utilisÃ© le modÃ¨le text-embedding-3-large d'OpenAI, mais vous pouvez utiliser n'importe quel modÃ¨le d'embedding moderne.

**Exemple d'embedding :**

```js
[
  -0.0002630692, -0.029749284, 0.010225477, -0.009224428, -0.0065269712,
  -0.002665544, 0.003214777, 0.04235309, -0.033162255, -0.00080789323,
  //...+1533 Ã©lÃ©ments
];
```

Chaque vecteur est une empreinte mathÃ©matique du texte, permettant la recherche de similaritÃ©.

## Ã‰tape 4 : Indexation et stockage des embeddings

Pour Ã©viter de rÃ©gÃ©nÃ©rer les embeddings plusieurs fois, je les ai stockÃ©s dans embeddings.json.

En production, vous voudriez probablement une base de donnÃ©es vectorielle telle que :

- Chroma
- Qdrant
- Pinecone
- FAISS, Weaviate, Milvus, etc.

Les bases de donnÃ©es vectorielles gÃ¨rent lâ€™indexation, la scalabilitÃ© et la recherche rapide. Mais pour mon prototype, un JSON local a trÃ¨s bien fonctionnÃ©.

## Ã‰tape 5 : Recherche avec similaritÃ© cosinus

Lorsquâ€™un utilisateur pose une question :

1. GÃ©nÃ©rer un embedding pour la requÃªte.
2. Le comparer Ã  tous les embeddings des documents en utilisant la **similaritÃ© cosinus**.
3. Ne garder que les N chunks les plus similaires.

La similaritÃ© cosinus mesure lâ€™angle entre deux vecteurs. Une correspondance parfaite obtient un score de **1.0**.

Ainsi, le systÃ¨me trouve les passages de documents les plus proches de la requÃªte.

## Ã‰tape 6 : Augmentation + GÃ©nÃ©ration

Voici la magie. Nous prenons les meilleurs chunks et les injectons dans le **prompt systÃ¨me** pour ChatGPT.

Cela signifie que le modÃ¨le rÃ©pond comme si ces extraits faisaient partie de la conversation.

Le rÃ©sultat : des rÃ©ponses prÃ©cises, **fondÃ©es sur la documentation**.

## Ã‰tape 7 : Enregistrement des requÃªtes utilisateur

Câ€™est la superpuissance cachÃ©e.

Chaque question posÃ©e est stockÃ©e. Au fil du temps, vous constituez un ensemble de donnÃ©es comprenant :

- Les questions les plus frÃ©quentes (idÃ©al pour les FAQ)
- Les questions sans rÃ©ponse (documentation manquante ou peu claire)
- Les demandes de fonctionnalitÃ©s dÃ©guisÃ©es en questions (Â« Est-ce que cela sâ€™intÃ¨gre avec X ? Â»)
- Les cas dâ€™utilisation Ã©mergents auxquels vous nâ€™aviez pas pensÃ©

Cela transforme votre assistant RAG en un **outil continu de recherche utilisateur**.

## Quel est le coÃ»t ?

Une objection courante au RAG est le coÃ»t. En pratique, câ€™est Ã©tonnamment peu cher :

- GÃ©nÃ©rer des embeddings pour environ 200 documents prend environ **5 minutes** et coÃ»te **1 Ã  2 euros**.
- La fonctionnalitÃ© de recherche dans la documentation est 100 % gratuite.
- Pour les requÃªtes, nous utilisons gpt-4o-latest sans le mode Â« rÃ©flexion Â». Sur Intlayer, nous observons environ **300 requÃªtes de chat par mois**, et la facture de lâ€™API OpenAI dÃ©passe rarement **10 $**.

En plus de cela, vous pouvez inclure le coÃ»t dâ€™hÃ©bergement.

## DÃ©tails de lâ€™implÃ©mentation

Stack :

- Monorepo : espace de travail pnpm
- Package doc : Node.js / TypeScript / API OpenAI
- Frontend : Next.js / React / Tailwind CSS
- Backend : route API Node.js / API OpenAI

Le package `@smart-doc/docs` est un package TypeScript qui gÃ¨re le traitement de la documentation. Lorsquâ€™un fichier markdown est ajoutÃ© ou modifiÃ©, le package inclut un script `build` qui reconstruit la liste de documentation dans chaque langue, gÃ©nÃ¨re des embeddings, et les stocke dans un fichier `embeddings.json`.

Pour le frontend, nous utilisons une application Next.js qui fournit :

- Rendu Markdown vers HTML
- Une barre de recherche pour trouver la documentation pertinente
- Une interface de chatbot pour poser des questions sur la documentation

Pour effectuer une recherche dans la documentation, lâ€™application Next.js inclut une route API qui appelle une fonction du package `@smart-doc/docs` pour rÃ©cupÃ©rer les segments de documentation correspondant Ã  la requÃªte. En utilisant ces segments, nous pouvons retourner une liste de pages de documentation pertinentes pour la recherche de lâ€™utilisateur.

Pour la fonctionnalitÃ© de chatbot, nous suivons le mÃªme processus de recherche mais injectons en plus les segments de documentation rÃ©cupÃ©rÃ©s dans le prompt envoyÃ© Ã  ChatGPT.

Voici un exemple de prompt envoyÃ© Ã  ChatGPT :

Prompt systÃ¨me :

```txt
Vous Ãªtes un assistant utile capable de rÃ©pondre aux questions concernant la documentation Intlayer.

Segments liÃ©s :

-----
docName: "getting-started"
docChunk: "1/3"
docUrl: "https://example.com/docs/fr/getting-started"
---

# Comment dÃ©marrer

...

-----
docName: "another-doc"
docChunk: "1/5"
docUrl: "https://example.com/docs/fr/another-doc"
---

# Un autre document

...
```

RequÃªte utilisateur :

```txt
Comment dÃ©marrer ?
```

Nous utilisons SSE pour diffuser la rÃ©ponse depuis la route API.

Comme mentionnÃ©, nous utilisons gpt-4-turbo sans mode "rÃ©flexion". Les rÃ©ponses sont pertinentes et la latence est faible.
Nous avons expÃ©rimentÃ© avec gpt-5, mais la latence Ã©tait trop Ã©levÃ©e (parfois jusqu'Ã  15 secondes pour une rÃ©ponse). Nous y reviendrons Ã  l'avenir.

ğŸ‘‰ [Essayez la dÃ©mo ici](https://intlayer.org/doc/why) ğŸ‘‰ [Consultez le modÃ¨le de code sur GitHub](https://github.com/aymericzip/smart_doc_RAG)

## Aller plus loin

Ce projet est une implÃ©mentation minimale. Mais vous pouvez l'Ã©tendre de nombreuses faÃ§ons :

- Serveur MCP â†’ la fonction de recherche dans la documentation vers un serveur MCP pour connecter la documentation Ã  n'importe quel assistant IA

- Bases de donnÃ©es vectorielles â†’ Ã©voluer jusqu'Ã  des millions de fragments de documentation
- LangChain / LlamaIndex â†’ frameworks prÃªts Ã  l'emploi pour les pipelines RAG
- Tableaux de bord analytiques â†’ visualiser les requÃªtes des utilisateurs et les points de douleur
- Recherche multi-source â†’ extraire non seulement des documents, mais aussi des entrÃ©es de base de donnÃ©es, articles de blog, tickets, etc.
- AmÃ©lioration des prompts â†’ reranking, filtrage, et recherche hybride (mot-clÃ© + sÃ©mantique)

## Limitations rencontrÃ©es

- Le dÃ©coupage en fragments et le chevauchement sont empiriques. Le bon Ã©quilibre (taille des fragments, pourcentage de chevauchement, nombre de fragments rÃ©cupÃ©rÃ©s) nÃ©cessite des itÃ©rations et des tests.
- Les embeddings ne sont pas rÃ©gÃ©nÃ©rÃ©s automatiquement lorsque les documents changent. Notre systÃ¨me rÃ©initialise les embeddings d'un fichier uniquement si le nombre de fragments diffÃ¨re de ce qui est stockÃ©.
- Dans ce prototype, les embeddings sont stockÃ©s en JSON. Cela fonctionne pour des dÃ©monstrations mais pollue Git. En production, une base de donnÃ©es ou un magasin vectoriel dÃ©diÃ© est prÃ©fÃ©rable.

## Pourquoi câ€™est important au-delÃ  de la documentation

La partie intÃ©ressante nâ€™est pas seulement le chatbot. Câ€™est la **boucle de rÃ©troaction**.

Avec RAG, vous ne vous contentez pas de rÃ©pondre :

- Vous apprenez ce qui embrouille les utilisateurs.
- Vous dÃ©couvrez quelles fonctionnalitÃ©s ils attendent.
- Vous adaptez votre stratÃ©gie produit en fonction des requÃªtes rÃ©elles.

**Exemple :**

Imaginez lancer une nouvelle fonctionnalitÃ© et voir instantanÃ©ment :

- 50 % des questions portent sur la mÃªme Ã©tape de configuration peu claire
- Les utilisateurs demandent Ã  plusieurs reprises une intÃ©gration que vous ne supportez pas encore
- Les gens recherchent des termes qui rÃ©vÃ¨lent un nouveau cas dâ€™usage

Câ€™est de **lâ€™intelligence produit** directement issue de vos utilisateurs.

## Conclusion

- Dans ce prototype, les embeddings sont stockÃ©s en JSON. Cela fonctionne pour des dÃ©monstrations mais pollue Git. En production, une base de donnÃ©es ou un magasin vectoriel dÃ©diÃ© est prÃ©fÃ©rable.

## Pourquoi câ€™est important au-delÃ  de la documentation

Lâ€™aspect intÃ©ressant ne se limite pas au chatbot. Câ€™est la **boucle de rÃ©troaction**.

Avec RAG, vous ne faites pas que rÃ©pondre :

- Vous apprenez ce qui embrouille les utilisateurs.
- Vous dÃ©couvrez quelles fonctionnalitÃ©s ils attendent.
- Vous adaptez votre stratÃ©gie produit en fonction des requÃªtes rÃ©elles.

**Exemple :**

Imaginez lancer une nouvelle fonctionnalitÃ© et voir instantanÃ©ment :

- 50 % des questions portent sur la mÃªme Ã©tape de configuration peu claire
- Les utilisateurs demandent Ã  plusieurs reprises une intÃ©gration que vous ne supportez pas encore
- Les gens recherchent des termes qui rÃ©vÃ¨lent un nouveau cas dâ€™usage

Câ€™est de **lâ€™intelligence produit** directement issue de vos utilisateurs.

## Conclusion

RAG est lâ€™une des maniÃ¨res les plus simples et puissantes de rendre les LLM pratiques. En combinant **rÃ©cupÃ©ration + gÃ©nÃ©ration**, vous pouvez transformer des docs statiques en un **assistant intelligent** et, en mÃªme temps, obtenir un flux continu dâ€™informations produit.

Pour moi, ce projet a montrÃ© que RAG nâ€™est pas quâ€™un simple tour technique. Câ€™est une faÃ§on de transformer la documentation en :

- un systÃ¨me de support interactif
- un canal de retour dâ€™expÃ©rience
- un outil de stratÃ©gie produit

ğŸ‘‰ [Essayez la dÃ©mo ici](https://intlayer.org/doc/why) ğŸ‘‰ [Consultez le modÃ¨le de code sur GitHub](https://github.com/aymericzip/smart_doc_RAG)

Et si vous expÃ©rimentez aussi avec RAG, jâ€™aimerais beaucoup savoir comment vous lâ€™utilisez.
