---
createdAt: 2025-09-10
updatedAt: 2025-09-10
title: Aufbau eines RAG-basierten Dokumentationsassistenten (Chunking, Embeddings und Suche)
description: Aufbau eines RAG-basierten Dokumentationsassistenten (Chunking, Embeddings und Suche)
keywords:
  - RAG
  - Dokumentation
  - Assistent
  - Chunking
  - Embeddings
  - Suche
slugs:
  - blog
  - rag-powered-documentation-assistant
---

# Aufbau eines RAG-basierten Dokumentationsassistenten (Chunking, Embeddings und Suche)

## Was Sie erhalten

Ich habe einen RAG-basierten Dokumentationsassistenten entwickelt und als Boilerplate verpackt, die Sie sofort verwenden k√∂nnen.

- Wird mit einer einsatzbereiten Anwendung geliefert (Next.js + OpenAI API)
- Beinhaltet eine funktionierende RAG-Pipeline (Chunking, Embeddings, Kosinus-√Ñhnlichkeit)
- Bietet eine vollst√§ndige Chatbot-Benutzeroberfl√§che, die in React erstellt wurde
- Alle UI-Komponenten sind vollst√§ndig mit Tailwind CSS bearbeitbar
- Protokolliert jede Benutzeranfrage, um fehlende Dokumentationen, Benutzerprobleme und Produktchancen zu identifizieren

üëâ¬†[Live-Demo](https://intlayer.org/doc/why) üëâ¬†[Code-Boilerplate](https://github.com/aymericzip/smart_doc_RAG)

## Einf√ºhrung

Wenn Sie jemals in der Dokumentation verloren waren und endlos nach einer Antwort gesucht haben, wissen Sie, wie m√ºhsam das sein kann. Dokumentationen sind n√ºtzlich, aber sie sind statisch, und die Suche f√ºhlt sich oft umst√§ndlich an.

Hier kommt **RAG (Retrieval-Augmented Generation)** ins Spiel. Anstatt Benutzer dazu zu zwingen, sich durch den Text zu w√ºhlen, k√∂nnen wir **Retrieval** (das Finden der richtigen Teile der Dokumentation) mit **Generation** (ein LLM erkl√§rt es nat√ºrlich) kombinieren.

In diesem Beitrag zeige ich Ihnen, wie ich einen RAG-gest√ºtzten Dokumentations-Chatbot entwickelt habe und wie dieser nicht nur den Nutzern hilft, schneller Antworten zu finden, sondern auch Produktteams eine neue M√∂glichkeit bietet, Benutzerprobleme zu verstehen.

## Warum RAG f√ºr Dokumentationen verwenden?

RAG ist aus gutem Grund zu einem beliebten Ansatz geworden: Es ist eine der praktischsten Methoden, um gro√üe Sprachmodelle tats√§chlich n√ºtzlich zu machen.

F√ºr Dokumentationen sind die Vorteile klar:

- Sofortige Antworten: Benutzer stellen Fragen in nat√ºrlicher Sprache und erhalten relevante Antworten.
- Besserer Kontext: Das Modell sieht nur die relevantesten Dokumentationsabschnitte, was Halluzinationen reduziert.
- Suche, die sich menschlich anf√ºhlt: eher wie Algolia + FAQ + Chatbot in einem.
- Feedback-Schleife: Durch das Speichern von Anfragen erkennen Sie, womit Benutzer wirklich Schwierigkeiten haben.

Dieser letzte Punkt ist entscheidend. Ein RAG-System beantwortet nicht nur Fragen, sondern zeigt Ihnen auch, was die Leute wirklich fragen. Das bedeutet:

- Sie entdecken fehlende Informationen in Ihren Dokumentationen.
- Sie sehen, wie Feature-Anfragen entstehen.
- Sie erkennen Muster, die sogar die Produktstrategie leiten k√∂nnen.

RAG ist also nicht nur ein Support-Tool. Es ist auch eine **Produktentdeckungsmaschine**.

## Wie die RAG-Pipeline funktioniert

Auf hoher Ebene sieht das Rezept, das ich verwendet habe, so aus:

1.  **Aufteilen der Dokumentation** Gro√üe Markdown-Dateien werden in Abschnitte (Chunks) aufgeteilt. Das Aufteilen erm√∂glicht es, nur die relevanten Teile der Dokumentation als Kontext bereitzustellen.
2.  **Erzeugen von Embeddings** Jeder Abschnitt wird mit der OpenAI-Embedding-API (text-embedding-3-large) oder einer Vektordatenbank (Chroma, Qdrant, Pinecone) in einen Vektor umgewandelt.
3.  **Indexierung & Speicherung** Embeddings werden in einer einfachen JSON-Datei gespeichert (f√ºr meine Demo), aber in der Produktion w√ºrden Sie wahrscheinlich eine Vektor-Datenbank verwenden.
4.  **Abruf (R in RAG)** Eine Benutzeranfrage wird eingebettet, die Kosinus√§hnlichkeit wird berechnet, und die am besten passenden Chunks werden abgerufen.
5.  **Erweiterung + Generierung (AG in RAG)** Diese Chunks werden in die Eingabeaufforderung f√ºr ChatGPT eingef√ºgt, sodass das Modell mit tats√§chlichem Dokumentationskontext antwortet.
6.  **Protokollierung von Anfragen f√ºr Feedback** Jede Benutzeranfrage wird gespeichert. Das ist Gold wert, um Schmerzpunkte, fehlende Dokumentationen oder neue Chancen zu verstehen.

## Schritt 1: Die Dokumentation lesen

Der erste Schritt war einfach: Ich brauchte eine M√∂glichkeit, einen docs/-Ordner nach allen .md-Dateien zu durchsuchen. Mit Node.js und glob habe ich den Inhalt jeder Markdown-Datei in den Speicher geladen.

Dies h√§lt die Pipeline flexibel: Anstatt Markdown k√∂nnten Sie die Dokumentation auch aus einer Datenbank, einem CMS oder sogar einer API abrufen.

## Schritt 2: Aufteilen der Dokumentation

Warum aufteilen? Weil Sprachmodelle **Kontextgrenzen** haben. Ein ganzes Buch voller Dokumentation auf einmal zu f√ºttern, funktioniert nicht.

Die Idee ist also, den Text in handhabbare Abschnitte (z. B. jeweils 500 Tokens) mit √úberlappungen (z. B. 100 Tokens) zu zerlegen. Die √úberlappung sorgt f√ºr Kontinuit√§t, damit am Abschnittsrand keine Bedeutung verloren geht.

**Beispiel:**

- Abschnitt 1 ‚Üí ‚Äû‚Ä¶die alte Bibliothek, die viele vergessen hatten. Ihre hohen Regale waren mit B√ºchern gef√ºllt‚Ä¶‚Äú
- Abschnitt 2 ‚Üí ‚Äû‚Ä¶Regale waren mit B√ºchern aus allen erdenklichen Genres gef√ºllt, die alle Geschichten fl√ºsterten‚Ä¶‚Äú

Die √úberlappung stellt sicher, dass beide Abschnitte gemeinsamen Kontext enthalten, sodass die Suche koh√§rent bleibt.

Dieser Kompromiss (Chunk-Gr√∂√üe vs. √úberlappung) ist entscheidend f√ºr die Effizienz von RAG:

- Zu klein ‚Üí es entsteht Rauschen.
- Zu gro√ü ‚Üí der Kontext wird zu umfangreich.

## Schritt 3: Erzeugen von Embeddings

Sobald die Dokumente in Chunks aufgeteilt sind, erzeugen wir **Embeddings** ‚Äî hochdimensionale Vektoren, die jeden Chunk repr√§sentieren.

Ich habe das OpenAI-Modell text-embedding-3-large verwendet, aber Sie k√∂nnen jedes moderne Embedding-Modell nutzen.

**Beispiel f√ºr ein Embedding:**

```js
[
  -0.0002630692, -0.029749284, 0.010225477, -0.009224428, -0.0065269712,
  -0.002665544, 0.003214777, 0.04235309, -0.033162255, -0.00080789323,
  //...+1533 Elemente
];
```

Jeder Vektor ist ein mathematischer Fingerabdruck des Textes und erm√∂glicht die √Ñhnlichkeitssuche.

## Schritt 4: Indexieren & Speichern der Embeddings

Um zu vermeiden, dass Embeddings mehrfach neu generiert werden, habe ich sie in embeddings.json gespeichert.

In der Produktion m√∂chten Sie wahrscheinlich eine Vektordatenbank wie folgende verwenden:

- Chroma
- Qdrant
- Pinecone
- FAISS, Weaviate, Milvus, etc.

Vektor-DBs √ºbernehmen Indexierung, Skalierbarkeit und schnelle Suche. F√ºr meinen Prototyp hat jedoch eine lokale JSON-Datei gut funktioniert.

## Schritt 5: Abruf mit Kosinus-√Ñhnlichkeit

Wenn ein Benutzer eine Frage stellt:

1.  Erzeugen Sie ein Embedding f√ºr die Anfrage.
2.  Vergleichen Sie es mit allen Dokument-Embeddings mittels **Kosinus-√Ñhnlichkeit**.
3.  Behalten Sie nur die N √§hnlichsten Chunks.

Die Kosinus-√Ñhnlichkeit misst den Winkel zwischen zwei Vektoren. Eine perfekte √úbereinstimmung erzielt **1,0**.

So findet das System die dem Query am n√§chsten liegenden Dokumentabschnitte.

## Schritt 6: Erweiterung + Generierung

Jetzt kommt die Magie. Wir nehmen die besten Chunks und injizieren sie in den **System-Prompt** f√ºr ChatGPT.

Das bedeutet, dass das Modell so antwortet, als w√§ren diese Abschnitte Teil des Gespr√§chs.

Das Ergebnis: pr√§zise, **dokumentenbasierte Antworten**.

## Schritt 7: Protokollierung der Benutzeranfragen

Das ist die versteckte Superkraft.

Jede gestellte Frage wird gespeichert. Im Laufe der Zeit entsteht so ein Datensatz mit:

- H√§ufigsten Fragen (ideal f√ºr FAQs)
- Unbeantworteten Fragen (Dokumentation fehlt oder ist unklar)
- Funktionsanfragen, die als Fragen getarnt sind (‚ÄûIntegriert es sich mit X?‚Äú)
- Aufkommenden Anwendungsf√§llen, die Sie nicht geplant hatten

Das verwandelt Ihren RAG-Assistenten in ein **kontinuierliches Nutzerforschungstool**.

## Was kostet das?

Ein h√§ufiger Einwand gegen RAG sind die Kosten. In der Praxis ist es √ºberraschend g√ºnstig:

- Die Erstellung von Embeddings f√ºr ca. 200 Dokumente dauert etwa **5 Minuten** und kostet **1‚Äì2 Euro**.
- Die Suchfunktion in den Dokumenten ist zu 100 % kostenlos.
- F√ºr Anfragen verwenden wir gpt-4o-latest ohne den ‚ÄûThinking‚Äú-Modus. Bei Intlayer sehen wir etwa **300 Chat-Anfragen pro Monat**, und die OpenAI-API-Rechnung √ºbersteigt selten **10 $**.

Dar√ºber hinaus k√∂nnen Sie die Hosting-Kosten einrechnen.

## Implementierungsdetails

Stack:

- Monorepo: pnpm Workspace
- Doc-Paket: Node.js / TypeScript / OpenAI API
- Frontend: Next.js / React / Tailwind CSS
- Backend: Node.js API-Route / OpenAI API

Das `@smart-doc/docs`-Paket ist ein TypeScript-Paket, das die Dokumentationsverarbeitung √ºbernimmt. Wenn eine Markdown-Datei hinzugef√ºgt oder ge√§ndert wird, enth√§lt das Paket ein `build`-Skript, das die Dokumentationsliste in jeder Sprache neu erstellt, Embeddings generiert und diese in einer `embeddings.json`-Datei speichert.

F√ºr das Frontend verwenden wir eine Next.js-Anwendung, die bietet:

- Markdown-zu-HTML-Rendering
- Eine Suchleiste, um relevante Dokumentation zu finden
- Eine Chatbot-Oberfl√§che, um Fragen zur Dokumentation zu stellen

Um eine Dokumentationssuche durchzuf√ºhren, enth√§lt die Next.js-Anwendung eine API-Route, die eine Funktion im `@smart-doc/docs`-Paket aufruft, um Dokumentationsabschnitte abzurufen, die zur Anfrage passen. Mithilfe dieser Abschnitte k√∂nnen wir eine Liste von Dokumentationsseiten zur√ºckgeben, die f√ºr die Suche des Benutzers relevant sind.

F√ºr die Chatbot-Funktionalit√§t folgen wir demselben Suchprozess, injizieren jedoch zus√§tzlich die abgerufenen Dokumentationsabschnitte in den Prompt, der an ChatGPT gesendet wird.

Hier ist ein Beispiel f√ºr einen an ChatGPT gesendeten Prompt:

System-Prompt:

```txt
You are a helpful assistant that can answer questions about the Intlayer documentation.

Related chunks :

-----
docName: "getting-started"
docChunk: "1/3"
docUrl: "https://example.com/docs/de/getting-started"
---

# Wie man anf√§ngt

...

-----
docName: "another-doc"
docChunk: "1/5"
docUrl: "https://example.com/docs/de/another-doc"
---

# Ein weiteres Dokument

...
```

Benutzeranfrage:

```txt
Wie f√§ngt man an?
```

Wir verwenden SSE, um die Antwort von der API-Route zu streamen.

Wie bereits erw√§hnt, verwenden wir gpt-4-turbo ohne den "Denkmodus". Die Antworten sind relevant und die Latenz gering.
Wir haben mit gpt-5 experimentiert, aber die Latenz war zu hoch (manchmal bis zu 15 Sekunden f√ºr eine Antwort). Das werden wir in Zukunft erneut pr√ºfen.

üëâ¬†[Hier die Demo ausprobieren](https://intlayer.org/doc/why) üëâ¬†[Den Code-Template auf GitHub ansehen](https://github.com/aymericzip/smart_doc_RAG)

## Weiterf√ºhrende M√∂glichkeiten

Dieses Projekt ist eine Minimalimplementierung. Aber Sie k√∂nnen es auf viele Arten erweitern:

- MCP-Server ‚Üí die Dokumentationssuchfunktion zu einem MCP-Server machen, um die Dokumentation mit jedem KI-Assistenten zu verbinden

- Vektor-Datenbanken ‚Üí Skalierung auf Millionen von Dokumentenabschnitten
- LangChain / LlamaIndex ‚Üí fertige Frameworks f√ºr RAG-Pipelines
- Analyse-Dashboards ‚Üí Visualisierung von Benutzeranfragen und Schmerzpunkten
- Multi-Source Retrieval ‚Üí nicht nur Dokumente abrufen, sondern auch Datenbankeintr√§ge, Blogbeitr√§ge, Tickets usw.
- Verbesserte Eingabeaufforderungen ‚Üí Neuranking, Filterung und hybride Suche (Schl√ºsselwort + semantisch)

## Begrenzungen, auf die wir gesto√üen sind

- Chunking und √úberlappung sind empirisch. Das richtige Gleichgewicht (Chunk-Gr√∂√üe, √úberlappungsprozentsatz, Anzahl der abgerufenen Chunks) erfordert Iteration und Tests.
- Embeddings werden nicht automatisch neu generiert, wenn sich die Dokumente √§ndern. Unser System setzt Embeddings f√ºr eine Datei nur zur√ºck, wenn sich die Anzahl der Chunks von der gespeicherten Anzahl unterscheidet.
- In diesem Prototyp werden Embeddings im JSON-Format gespeichert. Das funktioniert f√ºr Demos, verschmutzt aber das Git-Repository. In der Produktion ist eine Datenbank oder ein dedizierter Vektor-Speicher besser geeignet.

## Warum das √ºber die Dokumentation hinaus wichtig ist

Der interessante Teil ist nicht nur der Chatbot. Es ist der **Feedback-Loop**.

Mit RAG beantwortest du nicht nur:

- Du lernst, was die Nutzer verwirrt.
- Du entdeckst, welche Funktionen sie erwarten.
- Du passt deine Produktstrategie basierend auf echten Anfragen an.

**Beispiel:**

Stell dir vor, du f√ºhrst eine neue Funktion ein und siehst sofort:

- 50 % der Fragen betreffen denselben unklaren Einrichtungsschritt
- Nutzer fragen wiederholt nach einer Integration, die du noch nicht unterst√ºtzt
- Menschen suchen nach Begriffen, die einen neuen Anwendungsfall offenbaren

Das ist **Produktintelligenz** direkt von deinen Nutzern.

## Fazit

RAG ist eine der einfachsten und leistungsst√§rksten Methoden, um LLMs praktisch einsetzbar zu machen. Durch die Kombination von **Abruf + Generierung** k√∂nnen Sie statische Dokumentationen in einen **intelligenten Assistenten** verwandeln und gleichzeitig einen kontinuierlichen Strom von Produktinformationen gewinnen.

F√ºr mich hat dieses Projekt gezeigt, dass RAG nicht nur ein technischer Trick ist. Es ist eine M√∂glichkeit, Dokumentationen in Folgendes zu verwandeln:

- ein interaktives Unterst√ºtzungssystem
- einen Feedback-Kanal
- ein Werkzeug f√ºr die Produktstrategie

üëâ [Probieren Sie die Demo hier aus](https://intlayer.org/doc/why) üëâ [Sehen Sie sich die Code-Vorlage auf GitHub an](https://github.com/aymericzip/smart_doc_RAG)

Und wenn Sie auch mit RAG experimentieren, w√ºrde ich gerne h√∂ren, wie Sie es einsetzen.
