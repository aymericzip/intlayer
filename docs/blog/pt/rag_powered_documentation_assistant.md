---
createdAt: 2025-09-10
updatedAt: 2025-09-10
title: Construindo um Assistente de Documenta√ß√£o com RAG (Fragmenta√ß√£o, Embeddings e Busca)
description: Construindo um Assistente de Documenta√ß√£o com RAG (Fragmenta√ß√£o, Embeddings e Busca)
keywords:
  - RAG
  - Documenta√ß√£o
  - Assistente
  - Fragmenta√ß√£o
  - Embeddings
  - Busca
slugs:
  - blog
  - rag-powered-documentation-assistant
---

# Construindo um Assistente de Documenta√ß√£o com RAG (Fragmenta√ß√£o, Embeddings e Busca)

## O que voc√™ recebe

Eu constru√≠ um assistente de documenta√ß√£o com RAG e o empacotei em um boilerplate que voc√™ pode usar imediatamente.

- Vem com uma aplica√ß√£o pronta para uso (Next.js + OpenAI API)
- Inclui um pipeline RAG funcional (fragmenta√ß√£o, embeddings, similaridade cosseno)
- Fornece uma interface completa de chatbot constru√≠da em React
- Todos os componentes da interface s√£o totalmente edit√°veis com Tailwind CSS
- Registra todas as consultas dos usu√°rios para ajudar a identificar documenta√ß√£o faltante, pontos problem√°ticos dos usu√°rios e oportunidades de produto

üëâ¬†[Demo ao vivo](https://intlayer.org/doc/why) üëâ¬†[Boilerplate do c√≥digo](https://github.com/aymericzip/smart_doc_RAG)

## Introdu√ß√£o

Se voc√™ j√° se perdeu na documenta√ß√£o, rolando infinitamente em busca de uma resposta, sabe o qu√£o frustrante isso pode ser. Documenta√ß√µes s√£o √∫teis, mas s√£o est√°ticas e a busca nelas muitas vezes parece desajeitada.

√â a√≠ que entra o **RAG (Gera√ß√£o Aumentada por Recupera√ß√£o)**. Em vez de for√ßar os usu√°rios a vasculhar o texto, podemos combinar **recupera√ß√£o** (encontrar as partes certas da documenta√ß√£o) com **gera√ß√£o** (permitir que um LLM explique de forma natural).

Neste post, vou mostrar como constru√≠ um chatbot de documenta√ß√£o alimentado por RAG e como ele n√£o apenas ajuda os usu√°rios a encontrar respostas mais rapidamente, mas tamb√©m oferece √†s equipes de produto uma nova forma de entender os pontos problem√°ticos dos usu√°rios.

## Por que usar RAG para documenta√ß√£o?

RAG tornou-se uma abordagem popular por um motivo: √© uma das maneiras mais pr√°ticas de tornar os grandes modelos de linguagem realmente √∫teis.

Para documenta√ß√£o, os benef√≠cios s√£o claros:

- Respostas instant√¢neas: os usu√°rios perguntam em linguagem natural e recebem respostas relevantes.
- Melhor contexto: o modelo v√™ apenas as se√ß√µes mais relevantes da documenta√ß√£o, reduzindo alucina√ß√µes.
- Busca que parece humana: mais como Algolia + FAQ + chatbot, tudo em um.
- Ciclo de feedback: ao armazenar consultas, voc√™ descobre com o que os usu√°rios realmente t√™m dificuldades.

Esse √∫ltimo ponto √© crucial. Um sistema RAG n√£o apenas responde perguntas, ele mostra o que as pessoas est√£o perguntando. Isso significa:

- Voc√™ descobre informa√ß√µes faltantes na sua documenta√ß√£o.
- Voc√™ v√™ surgirem pedidos de funcionalidades.
- Voc√™ identifica padr√µes que podem at√© orientar a estrat√©gia do produto.

Portanto, RAG n√£o √© apenas uma ferramenta de suporte. √â tamb√©m um **motor de descoberta de produto**.

## Como funciona o pipeline RAG

Em um n√≠vel geral, aqui est√° a receita que usei:

1.  **Dividir a documenta√ß√£o em partes** Grandes arquivos Markdown s√£o divididos em partes. Dividir permite fornecer como contexto apenas as partes relevantes da documenta√ß√£o.
2.  **Gerar embeddings** Cada parte √© transformada em um vetor usando a API de embedding da OpenAI (text-embedding-3-large) ou um banco de dados vetorial (Chroma, Qdrant, Pinecone).
3.  **Indexa√ß√£o e armazenamento** Os embeddings s√£o armazenados em um arquivo JSON simples (para minha demonstra√ß√£o), mas em produ√ß√£o, provavelmente voc√™ usaria um banco de dados vetorial.
4.  **Recupera√ß√£o (R em RAG)** A consulta do usu√°rio √© transformada em embedding, a similaridade cosseno √© calculada, e os chunks mais relevantes s√£o recuperados.
5.  **Aumento + Gera√ß√£o (AG em RAG)** Esses chunks s√£o injetados no prompt para o ChatGPT, para que o modelo responda com o contexto real da documenta√ß√£o.
6.  **Registro de consultas para feedback** Cada consulta do usu√°rio √© armazenada. Isso √© ouro para entender pontos problem√°ticos, documenta√ß√£o faltante ou novas oportunidades.

## Passo 1: Lendo a Documenta√ß√£o

O primeiro passo foi simples: eu precisava de uma forma de escanear uma pasta docs/ para todos os arquivos .md. Usando Node.js e glob, eu busquei o conte√∫do de cada arquivo Markdown para a mem√≥ria.

Isso mant√©m o pipeline flex√≠vel: em vez de Markdown, voc√™ poderia buscar documentos de um banco de dados, um CMS ou at√© mesmo uma API.

## Passo 2: Dividindo a Documenta√ß√£o em Partes

Por que dividir em partes? Porque os modelos de linguagem t√™m **limites de contexto**. Aliment√°-los com um livro inteiro de documenta√ß√£o n√£o funciona.

Ent√£o, a ideia √© quebrar o texto em partes gerenci√°veis (por exemplo, 500 tokens cada) com sobreposi√ß√£o (por exemplo, 100 tokens). A sobreposi√ß√£o garante continuidade para que voc√™ n√£o perca o significado nas fronteiras das partes.

**Exemplo:**

- Parte 1 ‚Üí ‚Äú‚Ä¶a velha biblioteca que muitos haviam esquecido. Suas prateleiras imponentes estavam cheias de livros‚Ä¶‚Äù
- Parte 2 ‚Üí ‚Äú‚Ä¶as prateleiras estavam cheias de livros de todos os g√™neros imagin√°veis, cada um sussurrando hist√≥rias‚Ä¶‚Äù

A sobreposi√ß√£o garante que ambas as partes contenham contexto compartilhado, para que a recupera√ß√£o permane√ßa coerente.

Esse equil√≠brio (tamanho do chunk vs sobreposi√ß√£o) √© fundamental para a efici√™ncia do RAG:

- Muito pequeno ‚Üí voc√™ obt√©m ru√≠do.
- Muito grande ‚Üí voc√™ estoura o tamanho do contexto.

## Passo 3: Gerando Embeddings

Uma vez que os documentos s√£o divididos em chunks, geramos **embeddings** ‚Äî vetores de alta dimens√£o que representam cada chunk.

Usei o modelo text-embedding-3-large da OpenAI, mas voc√™ pode usar qualquer modelo moderno de embedding.

**Exemplo de embedding:**

```js
[
  -0.0002630692, -0.029749284, 0.010225477, -0.009224428, -0.0065269712,
  -0.002665544, 0.003214777, 0.04235309, -0.033162255, -0.00080789323,
  //...+1533 elementos
];
```

Cada vetor √© uma impress√£o digital matem√°tica do texto, permitindo a busca por similaridade.

## Passo 4: Indexando e Armazenando Embeddings

Para evitar regenerar embeddings v√°rias vezes, eu os armazenei em embeddings.json.

Em produ√ß√£o, voc√™ provavelmente vai querer um banco de dados vetorial como:

- Chroma
- Qdrant
- Pinecone
- FAISS, Weaviate, Milvus, etc.

Bancos de dados vetoriais lidam com indexa√ß√£o, escalabilidade e busca r√°pida. Mas para meu prot√≥tipo, um JSON local funcionou bem.

## Passo 5: Recupera√ß√£o com Similaridade Cosseno

Quando um usu√°rio faz uma pergunta:

1. Gere um embedding para a consulta.
2. Compare-o com todos os embeddings dos documentos usando **similaridade cosseno**.
3. Mantenha apenas os N chunks mais similares.

A similaridade cosseno mede o √¢ngulo entre dois vetores. Uma correspond√™ncia perfeita tem pontua√ß√£o **1.0**.

Dessa forma, o sistema encontra as passagens do documento mais pr√≥ximas da consulta.

## Passo 6: Aumento + Gera√ß√£o

Agora vem a m√°gica. Pegamos os chunks principais e os injetamos no **prompt do sistema** para o ChatGPT.

Isso significa que o modelo responde como se esses trechos fizessem parte da conversa.

O resultado: respostas precisas e **baseadas na documenta√ß√£o**.

## Passo 7: Registro das Consultas dos Usu√°rios

Este √© o superpoder oculto.

Cada pergunta feita √© armazenada. Com o tempo, voc√™ constr√≥i um conjunto de dados de:

- Perguntas mais frequentes (√≥timo para FAQs)
- Perguntas sem resposta (documenta√ß√£o ausente ou pouco clara)
- Solicita√ß√µes de funcionalidades disfar√ßadas de perguntas (‚ÄúIntegra com X?‚Äù)
- Casos de uso emergentes que voc√™ n√£o havia planejado

Isso transforma seu assistente RAG em uma **ferramenta cont√≠nua de pesquisa com usu√°rios**.

## Quanto Custa?

Uma obje√ß√£o comum ao RAG √© o custo. Na pr√°tica, √© surpreendentemente barato:

- Gerar embeddings para cerca de 200 documentos leva cerca de **5 minutos** e custa **1‚Äì2 euros**.
- O recurso de busca na documenta√ß√£o √© 100% gratuito.
- Para consultas, usamos o gpt-4o-latest sem o modo ‚Äúthinking‚Äù. No Intlayer, vemos cerca de **300 consultas de chat por m√™s**, e a fatura da API OpenAI raramente ultrapassa **10 d√≥lares**.

Al√©m disso, voc√™ pode incluir o custo de hospedagem.

## Detalhes da Implementa√ß√£o

Stack:

- Monorepo: workspace pnpm
- Pacote de documenta√ß√£o: Node.js / TypeScript / API OpenAI
- Frontend: Next.js / React / Tailwind CSS
- Backend: rota API Node.js / API OpenAI

O pacote `@smart-doc/docs` √© um pacote TypeScript que lida com o processamento da documenta√ß√£o. Quando um arquivo markdown √© adicionado ou modificado, o pacote inclui um script `build` que reconstr√≥i a lista de documenta√ß√£o em cada idioma, gera embeddings e os armazena em um arquivo `embeddings.json`.

Para o frontend, usamos uma aplica√ß√£o Next.js que fornece:

- Renderiza√ß√£o de Markdown para HTML
- Uma barra de pesquisa para encontrar documenta√ß√£o relevante
- Uma interface de chatbot para fazer perguntas sobre a documenta√ß√£o

Para realizar uma busca na documenta√ß√£o, a aplica√ß√£o Next.js inclui uma rota de API que chama uma fun√ß√£o do pacote `@smart-doc/docs` para recuperar fragmentos de documenta√ß√£o que correspondem √† consulta. Usando esses fragmentos, podemos retornar uma lista de p√°ginas de documenta√ß√£o relevantes para a busca do usu√°rio.

Para a funcionalidade do chatbot, seguimos o mesmo processo de busca, mas al√©m disso, injetamos os fragmentos de documenta√ß√£o recuperados no prompt enviado ao ChatGPT.

Aqui est√° um exemplo de um prompt enviado ao ChatGPT:

Prompt do sistema:

```txt
Voc√™ √© um assistente prestativo que pode responder perguntas sobre a documenta√ß√£o do Intlayer.

Fragmentos relacionados:

-----
docName: "getting-started"
docChunk: "1/3"
docUrl: "https://example.com/docs/pt/getting-started"
---

# Como come√ßar

...

-----
docName: "another-doc"
docChunk: "1/5"
docUrl: "https://example.com/docs/pt/another-doc"
---

# Outro documento

...
```

Consulta do usu√°rio :

```txt
Como come√ßar?
```

Usamos SSE para transmitir a resposta da rota da API.

Como mencionado, usamos gpt-4-turbo sem o modo "thinking". As respostas s√£o relevantes e a lat√™ncia √© baixa.
Experimentamos com o gpt-5, mas a lat√™ncia era muito alta (√†s vezes at√© 15 segundos para uma resposta). Mas revisaremos isso no futuro.

üëâ¬†[Experimente a demonstra√ß√£o aqui](https://intlayer.org/doc/why) üëâ¬†[Confira o modelo de c√≥digo no GitHub](https://github.com/aymericzip/smart_doc_RAG)

## Indo Al√©m

Este projeto √© uma implementa√ß√£o m√≠nima. Mas voc√™ pode estend√™-lo de v√°rias maneiras:

- Servidor MCP ‚Üí a fun√ß√£o de pesquisa de documenta√ß√£o para um servidor MCP para conectar a documenta√ß√£o a qualquer assistente de IA

- Bancos de Dados Vetoriais ‚Üí escalar para milh√µes de peda√ßos de documenta√ß√£o
- LangChain / LlamaIndex ‚Üí frameworks prontos para pipelines RAG
- Pain√©is de an√°lise ‚Üí visualizar consultas dos usu√°rios e pontos problem√°ticos
- Recupera√ß√£o multi-fonte ‚Üí buscar n√£o apenas documentos, mas entradas de banco de dados, posts de blog, tickets, etc.
- Melhoria no prompting ‚Üí reclassifica√ß√£o, filtragem e busca h√≠brida (palavra-chave + sem√¢ntica)

## Limita√ß√µes que Encontramos

- A segmenta√ß√£o e sobreposi√ß√£o s√£o emp√≠ricas. O equil√≠brio correto (tamanho do peda√ßo, porcentagem de sobreposi√ß√£o, n√∫mero de peda√ßos recuperados) requer itera√ß√£o e testes.
- Embeddings n√£o s√£o regenerados automaticamente quando os documentos mudam. Nosso sistema redefine os embeddings para um arquivo somente se o n√∫mero de peda√ßos for diferente do que est√° armazenado.
- Neste prot√≥tipo, os embeddings s√£o armazenados em JSON. Isso funciona para demonstra√ß√µes, mas polui o Git. Em produ√ß√£o, um banco de dados ou um armazenamento vetorial dedicado √© melhor.

## Por Que Isso Importa Al√©m da Documenta√ß√£o

A parte interessante n√£o √© apenas o chatbot. √â o **ciclo de feedback**.

Com RAG, voc√™ n√£o apenas responde:

- Voc√™ aprende o que confunde os usu√°rios.
- Voc√™ descobre quais recursos eles esperam.
- Voc√™ adapta sua estrat√©gia de produto com base em consultas reais.

**Exemplo:**

Imagine lan√ßar um novo recurso e ver instantaneamente:

- 50% das perguntas s√£o sobre o mesmo passo de configura√ß√£o pouco claro
- Usu√°rios pedem repetidamente por uma integra√ß√£o que voc√™ ainda n√£o suporta
- Pessoas buscam termos que revelam um novo caso de uso

Isso √© **intelig√™ncia de produto** diretamente dos seus usu√°rios.

## Conclus√£o

RAG √© uma das formas mais simples e poderosas de tornar os LLMs pr√°ticos. Ao combinar **recupera√ß√£o + gera√ß√£o**, voc√™ pode transformar documentos est√°ticos em um **assistente inteligente** e, ao mesmo tempo, obter um fluxo cont√≠nuo de insights sobre o produto.

Para mim, este projeto mostrou que RAG n√£o √© apenas um truque t√©cnico. √â uma forma de transformar a documenta√ß√£o em:

- um sistema de suporte interativo
- um canal de feedback
- uma ferramenta de estrat√©gia de produto

üëâ [Experimente a demonstra√ß√£o aqui](https://intlayer.org/doc/why) üëâ [Confira o modelo de c√≥digo no GitHub](https://github.com/aymericzip/smart_doc_RAG)

E se voc√™ tamb√©m estiver experimentando com RAG, adoraria saber como est√° usando.
